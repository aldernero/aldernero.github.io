
[{"content":"In my previous post I described how to setup a MySQL environment on TrueNAS (or via docker more generally). I wanted to kick the tires on the database by loading it with an interesting dataset. I chose IMDb\u0026rsquo;s public dataset. This post details the approach I took to ingesting the data and some basic queries to check that everything works.\nIMDb Non-Commercial Datasets # The datasets come as compressed TSV (tab separated) files, along with a description of each column:\nOnce downloaded and uncompressed, this was the set of files and sizes to work with:\n\u0026gt; ls -lh *.tsv -rw-r--r-- 1 vernon vernon 862M Sep 5 15:21 name.basics.tsv -rw-r--r-- 1 vernon vernon 2.5G Sep 5 15:21 title.akas.tsv -rw-r--r-- 1 vernon vernon 981M Sep 5 15:22 title.basics.tsv -rw-r--r-- 1 vernon vernon 375M Sep 5 15:22 title.crew.tsv -rw-r--r-- 1 vernon vernon 229M Sep 5 15:22 title.episode.tsv -rw-r--r-- 1 vernon vernon 4.0G Sep 5 15:22 title.principals.tsv -rw-r--r-- 1 vernon vernon 27M Sep 5 15:22 title.ratings.tsv The description gives the expected data type for each column, but it doesn\u0026rsquo;t say how long strings ought to be. Since the dataset is static, I first wrote a program to parse the data and find the maximum length for each column. The program also checks for empty values and counts the total rows. This info was useful in the following step of defining the database tables.\nimport glob from dataclasses import dataclass, field @dataclass class Column: name: str has_nulls: bool = False max_width: int = 0 @dataclass class Table: name: str columns: dict = field(default_factory=dict) rows: int = 0 def main(): fnames = glob.glob(\u0026#34;*tsv\u0026#34;) tables = {} for fname in fnames: table = Table(fname) with open(fname, encoding=\u0026#34;utf-8\u0026#34;) as f: keys = f.readline().strip().split(\u0026#34;\\t\u0026#34;) for key in keys: table.columns[key] = Column(key) for line in f: row = line.strip().split(\u0026#34;\\t\u0026#34;) for i, item in enumerate(row): k = keys[i] if item is None: print(\u0026#34;error\u0026#34;, len(row), row) continue if item == \u0026#34;\\\\N\u0026#34; or item is None: table.columns[k].has_nulls = True continue if len(item) \u0026gt; table.columns[k].max_width: table.columns[k].max_width = len(item) table.rows += 1 tables[fname] = table print(table.name) for v in table.columns.values(): print(f\u0026#34;\\t{v}\u0026#34;) print(f\u0026#34;\\t{table.rows} rows\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: main() It took several minutes to run, but eventually gave me the data I was looking for:\n\u0026gt; python parse_tsv.py name.basics.tsv Column(name=\u0026#39;nconst\u0026#39;, has_nulls=False, max_width=10) Column(name=\u0026#39;primaryName\u0026#39;, has_nulls=True, max_width=105) Column(name=\u0026#39;birthYear\u0026#39;, has_nulls=True, max_width=4) Column(name=\u0026#39;deathYear\u0026#39;, has_nulls=True, max_width=4) Column(name=\u0026#39;primaryProfession\u0026#39;, has_nulls=True, max_width=66) Column(name=\u0026#39;knownForTitles\u0026#39;, has_nulls=True, max_width=43) 14692473 rows title.akas.tsv Column(name=\u0026#39;titleId\u0026#39;, has_nulls=False, max_width=10) Column(name=\u0026#39;ordering\u0026#39;, has_nulls=False, max_width=3) Column(name=\u0026#39;title\u0026#39;, has_nulls=False, max_width=831) Column(name=\u0026#39;region\u0026#39;, has_nulls=True, max_width=4) Column(name=\u0026#39;language\u0026#39;, has_nulls=True, max_width=3) Column(name=\u0026#39;types\u0026#39;, has_nulls=True, max_width=20) Column(name=\u0026#39;attributes\u0026#39;, has_nulls=True, max_width=62) Column(name=\u0026#39;isOriginalTitle\u0026#39;, has_nulls=False, max_width=1) 53055800 rows title.basics.tsv Column(name=\u0026#39;tconst\u0026#39;, has_nulls=False, max_width=10) Column(name=\u0026#39;titleType\u0026#39;, has_nulls=False, max_width=12) Column(name=\u0026#39;primaryTitle\u0026#39;, has_nulls=False, max_width=458) Column(name=\u0026#39;originalTitle\u0026#39;, has_nulls=False, max_width=458) Column(name=\u0026#39;isAdult\u0026#39;, has_nulls=False, max_width=1) Column(name=\u0026#39;startYear\u0026#39;, has_nulls=True, max_width=4) Column(name=\u0026#39;endYear\u0026#39;, has_nulls=True, max_width=4) Column(name=\u0026#39;runtimeMinutes\u0026#39;, has_nulls=True, max_width=7) Column(name=\u0026#39;genres\u0026#39;, has_nulls=True, max_width=32) 11884044 rows title.crew.tsv Column(name=\u0026#39;tconst\u0026#39;, has_nulls=False, max_width=10) Column(name=\u0026#39;directors\u0026#39;, has_nulls=True, max_width=5320) Column(name=\u0026#39;writers\u0026#39;, has_nulls=True, max_width=14089) 11884044 rows title.episode.tsv Column(name=\u0026#39;tconst\u0026#39;, has_nulls=False, max_width=10) Column(name=\u0026#39;parentTconst\u0026#39;, has_nulls=False, max_width=10) Column(name=\u0026#39;seasonNumber\u0026#39;, has_nulls=True, max_width=4) Column(name=\u0026#39;episodeNumber\u0026#39;, has_nulls=True, max_width=5) 9150593 rows title.principals.tsv Column(name=\u0026#39;tconst\u0026#39;, has_nulls=False, max_width=10) Column(name=\u0026#39;ordering\u0026#39;, has_nulls=False, max_width=2) Column(name=\u0026#39;nconst\u0026#39;, has_nulls=False, max_width=10) Column(name=\u0026#39;category\u0026#39;, has_nulls=False, max_width=19) Column(name=\u0026#39;job\u0026#39;, has_nulls=True, max_width=290) Column(name=\u0026#39;characters\u0026#39;, has_nulls=True, max_width=463) 94519567 rows title.ratings.tsv Column(name=\u0026#39;tconst\u0026#39;, has_nulls=False, max_width=10) Column(name=\u0026#39;averageRating\u0026#39;, has_nulls=False, max_width=4) Column(name=\u0026#39;numVotes\u0026#39;, has_nulls=False, max_width=7) 1609544 rows Creating the tables # I created one table per file, using the data from the parsing program to decide column widths and whether a column should allow NULL values. I also tried to use sensible primary keys so that joins and queries would be fast later on.\nuse movies; CREATE TABLE name_basics ( nconst VARCHAR(10) PRIMARY KEY, primary_name VARCHAR(255), birth_year INT, death_year INT, primary_profession JSON, known_for JSON ); CREATE TABLE title_basics ( tconst VARCHAR(10) PRIMARY KEY, movie_type VARCHAR(255), primary_title VARCHAR(1024), original_title VARCHAR(1024), is_adult BOOLEAN, start_year INT NOT NULL, end_year INT, runtime_minutes INT, genres JSON ); CREATE TABLE title_akas ( tconst VARCHAR(10), ordering INT, title VARCHAR(1024), region VARCHAR(255), language VARCHAR(255), types JSON, attributes JSON, is_original_title BOOLEAN, PRIMARY KEY (movie_id, ordering) ) ; CREATE TABLE title_crew ( tconst VARCHAR(10) PRIMARY KEY, directors JSON, writers JSON ); CREATE TABLE title_episodes ( tconst VARCHAR(10) PRIMARY KEY, parent_tconst VARCHAR(10), season_num INT, episode_num INT ); CREATE TABLE title_principals ( tconst VARCHAR(10), ordering INT, nconst VARCHAR(10), category VARCHAR(255), job VARCHAR(512), characters JSON, PRIMARY KEY (movie_id, ordering) ); CREATE TABLE title_ratings ( tconst VARCHAR(10) PRIMARY KEY, average_rating DECIMAL(3, 1), num_votes INT ); Ingesting the data # With the tables created, I needed another program to iterate through the TSV files and insert each row into the appropriate table.\nimport json import re import sys import time import mysql.connector # from stackoverflow # https://stackoverflow.com/questions/1175208/elegant-python-function-to-convert-camelcase-to-snake-case def camel_to_snake(name: str) -\u0026gt; str: pattern = re.compile(r\u0026#39;(?\u0026lt;!^)(?=[A-Z])\u0026#39;) name = pattern.sub(\u0026#39;_\u0026#39;, name).lower() return name def fname_to_table(fname: str) -\u0026gt; str: parts = fname.split(\u0026#34;.\u0026#34;) return \u0026#34;_\u0026#34;.join(parts[:-1]) def fields_to_columns(fields: list) -\u0026gt; str: result = \u0026#34;(\u0026#34; result += \u0026#34;,\u0026#34;.join(map(lambda x: f\u0026#34;`{camel_to_snake(x)}`\u0026#34;, fields)) result += \u0026#34;)\u0026#34; return result def format_insert_stmt(fname, fields): tbl = fname_to_table(fname) cols = fields_to_columns(fields) vals = \u0026#34;, \u0026#34;.join([\u0026#34;%s\u0026#34; for _ in range(len(fields))]) return f\u0026#34;INSERT INTO {tbl} {cols} VALUES ({vals})\u0026#34; def format_value(fname, column, data): if column in table_metadata[fname]: return table_metadata[fname][column](data) else: return check_for_null(data) check_for_null = lambda s: None if s == \u0026#34;\\\\N\u0026#34; else s to_json_str = lambda s: None if (s == \u0026#34;\\\\N\u0026#34; or s is None) else json.dumps(s.split(\u0026#34;,\u0026#34;)) files = [ \u0026#34;name.basics.tsv\u0026#34;, \u0026#34;title.akas.tsv\u0026#34;, \u0026#34;title.basics.tsv\u0026#34;, \u0026#34;title.crew.tsv\u0026#34;, \u0026#34;title.episode.tsv\u0026#34;, \u0026#34;title.principals.tsv\u0026#34;, \u0026#34;title.ratings.tsv\u0026#34;, ] table_metadata = { \u0026#34;name.basics.tsv\u0026#34;: { \u0026#34;primaryProfession\u0026#34;: to_json_str, \u0026#34;knownForTitles\u0026#34;: to_json_str, \u0026#34;rows\u0026#34;: 14692473 }, \u0026#34;title.akas.tsv\u0026#34;: { \u0026#34;types\u0026#34;: to_json_str, \u0026#34;attributes\u0026#34;: to_json_str, \u0026#34;rows\u0026#34;: 53055800 }, \u0026#34;title.basics.tsv\u0026#34;: { \u0026#34;genres\u0026#34;: to_json_str, \u0026#34;filter\u0026#34;: lambda d: d[4] == \u0026#34;1\u0026#34;, # only keeps isAdult == 0 \u0026#34;rows\u0026#34;: 11884044 }, \u0026#34;title.crew.tsv\u0026#34;: { \u0026#34;directors\u0026#34;: to_json_str, \u0026#34;writers\u0026#34;: to_json_str, \u0026#34;rows\u0026#34;: 11884044 }, \u0026#34;title.episode.tsv\u0026#34;: { \u0026#34;rows\u0026#34;: 9150593 }, \u0026#34;title.principals.tsv\u0026#34;: { \u0026#34;characters\u0026#34;: to_json_str, \u0026#34;rows\u0026#34;: 94519567 }, \u0026#34;title.ratings.tsv\u0026#34;: { \u0026#34;rows\u0026#34;: 1609544 }, } def main(): try: mydb = mysql.connector.connect( host=\u0026#34;\u0026lt;redacted\u0026gt;\u0026#34;, user=\u0026#34;\u0026lt;redacted\u0026gt;\u0026#34;, passwd=\u0026#34;\u0026lt;super-redacted\u0026gt;\u0026#34;, database=\u0026#34;movies\u0026#34; ) mycursor = mydb.cursor() for fname in files: with open(fname) as f: fields = f.readline().strip().split(\u0026#34;\\t\u0026#34;) columns = list(map(lambda x: f\u0026#34;`{camel_to_snake(x)}`\u0026#34;, fields)) num_rows = table_metadata[fname][\u0026#34;rows\u0026#34;] sql = format_insert_stmt(fname, fields) rows = 0 for line in f: rows += 1 row = line.strip().split(\u0026#34;\\t\u0026#34;) if \u0026#34;filter\u0026#34; in table_metadata[fname]: if table_metadata[fname][\u0026#34;filter\u0026#34;](row): continue vals = [] for i, item in enumerate(fields): vals.append(format_value(fname, item, row[i])) mycursor.execute(sql, tuple(vals)) if rows % 100000 == 0: mydb.commit() progress = 100 * rows / num_rows print(f\u0026#34;[ {fname} ]( {int(time.time())} ): {progress:.2f} %\u0026#34;) except mysql.connector.Error as err: print(err) sys.exit(1) finally: if \u0026#39;mydb\u0026#39; in locals() and mydb.is_connected(): mycursor.close() mydb.close() if __name__ == \u0026#34;__main__\u0026#34;: main() I chose to commit to the db every 100000 rows, but even so, this process took quite a while, a couple of hours IIRC. I wasn\u0026rsquo;t too concerned with optimizing any part of the ingest as this was a one time thing on a static dataset.\nLet\u0026rsquo;s query some data # I used Grafana to query the database. In the last blog post I created a data source for the mysql database and created a grafana database user that only has select permissions.\nAs a quick test let\u0026rsquo;s see a few actors who where born in 1979:\nThat worked nicely, but this is something that\u0026rsquo;s easy to search on IMDb itself. Let\u0026rsquo;s try something that would be more difficult to find on the website, and that requires some table joins. I think the 90s were a great decade for comedy, especially the mid-90s. Let\u0026rsquo;s craft a query that will tell us the top 10 rated US comedies from 1994-1996:\nSELECT tb.primary_title AS \u0026#34;Title\u0026#34;, tb.start_year AS \u0026#34;Year\u0026#34;, tr.average_rating AS \u0026#34;Rating\u0026#34;, tr.num_votes AS \u0026#34;Votes\u0026#34;, nb.primary_name AS \u0026#34;Starring\u0026#34; FROM title_basics tb JOIN title_ratings tr ON tb.tconst = tr.tconst JOIN title_principals tp ON tb.tconst = tp.tconst JOIN name_basics nb ON tp.nconst = nb.nconst WHERE tb.start_year BETWEEN 1994 AND 1996 AND tb.title_type = \u0026#34;movie\u0026#34; AND JSON_LENGTH(tb.genres) = 1 AND JSON_EXTRACT(tb.genres, \u0026#34;$[0]\u0026#34;) = \u0026#34;Comedy\u0026#34; AND tp.ordering = 1 -- Get the first/main actor AND tp.category = \u0026#39;actor\u0026#39; -- Ensure it\u0026#39;s an actor, not director/writer AND tr.num_votes \u0026gt;= 10000 -- Minimum vote threshold AND EXISTS ( SELECT 1 FROM title_akas ta WHERE ta.tconst = tb.tconst AND ta.region = \u0026#39;US\u0026#39; ) -- Filter for movies that have US region entries ORDER BY tr.average_rating DESC, tr.num_votes DESC LIMIT 10; This gives us the result\nNice. This was fun.\n* Photo by Samuel Regan-Asante on Unsplash\n","date":"29 September 2025","externalUrl":null,"permalink":"/blog/imdb-mysql/","section":"Blogs","summary":"","title":"Adding IMDb's Public Dataset to MySQL","type":"blog"},{"content":"","date":"29 September 2025","externalUrl":null,"permalink":"/blog/","section":"Blogs","summary":"","title":"Blogs","type":"blog"},{"content":"","date":"29 September 2025","externalUrl":null,"permalink":"/tags/grafana/","section":"Tags","summary":"","title":"Grafana","type":"tags"},{"content":"","date":"29 September 2025","externalUrl":null,"permalink":"/tags/movies/","section":"Tags","summary":"","title":"Movies","type":"tags"},{"content":"","date":"29 September 2025","externalUrl":null,"permalink":"/tags/sql/","section":"Tags","summary":"","title":"Sql","type":"tags"},{"content":"","date":"29 September 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"29 September 2025","externalUrl":null,"permalink":"/","section":"Vernon's Miscellany","summary":"","title":"Vernon's Miscellany","type":"page"},{"content":"","date":"7 September 2025","externalUrl":null,"permalink":"/tags/mysql/","section":"Tags","summary":"","title":"Mysql","type":"tags"},{"content":"TL;DR TrueNAS makes it easy to deploy containerized environments as \u0026ldquo;Apps\u0026rdquo;. This post describes how to build a custom app that deploys MySQL, phpMyAdmin, mysqld_exporter, and how to monitor the database with Grafana.\nBackground # I recently upgraded my custom built storage server from TrueNAS Core to TrueNAS SCALE, now just called TrueNAS (Community Edition). Firstly, I was impressed with how seemless the upgrade was. Especially considering it\u0026rsquo;s switching from FreeBSD to Debian, and from jails to docker. Secondly, I like the new UI, it\u0026rsquo;s more modern and quite responsive.\nOne of the reasons I wanted to upgrade, besides Core being EOL, was to see what the containerized experience is like. With Core there were plugins, both official from iXsystems and community created, that used jails and FreeBSD utilities to deploy things like Minio and Nextcloud. With SCALE, the name has changed to \u0026ldquo;Apps\u0026rdquo; and under the covers docker and docker compose are used to deploy containerized applications.\nJust like before, there are lots of predefined apps. Users can browse or search under Apps -\u0026gt; Discover, which shows mix of stable and community apps. For now I\u0026rsquo;ve installed a few and they worked pretty well out of the box.\nIt\u0026rsquo;s also possible to deploy custom apps via Apps -\u0026gt; Discover -\u0026gt; three dots in upper right -\u0026gt; Install via YAML. That\u0026rsquo;s what I used to install the MySQL environment, described in the next section.\nCrafting a database environment # The goal was to create an app with 3 workloads:\nMySQL - the venerable RDMS phpMyAdmin - a web-based GUI for admistering the database mysqld_exporter - a Prometheus exporter for monitoring the database First I created a new Dataset on my fast vdev /mnt/fast/mysql and set apps as the owner and group. Next via a shell I created a directory mysql_data and two files for configuration:\napp_compose.yaml: Docker compose file exporter_config.toml: config file for Prometheus exporter Let\u0026rsquo;s look at the Docker compose file\nversion: \u0026#39;3.8\u0026#39; services: mysql: image: mysql:latest restart: always environment: MYSQL_ROOT_PASSWORD: MY-DB-ROOT-PASSWORD MYSQL_DATABASE: MY-DB-NAME MYSQL_USER: MY-DB-USER MYSQL_PASSWORD: MY-DB-USER-PASSWORD volumes: - ./mysql_data:/var/lib/mysql ports: - \u0026#34;3306:3306\u0026#34; phpmyadmin: image: phpmyadmin/phpmyadmin:latest restart: always depends_on: - mysql environment: PMA_HOST: mysql MYSQL_ROOT_PASSWORD: MY-DB-ROOT-PASSWORD ports: - \u0026#34;6060:80\u0026#34; mysqld-exporter: image: prom/mysqld-exporter:latest restart: always command: [\u0026#34;mysqld_exporter\u0026#34;, \u0026#34;--log.level\u0026#34;, \u0026#34;debug\u0026#34;, \u0026#34;--config.my-cnf\u0026#34;, \u0026#34;/.my.cnf\u0026#34;, \u0026#34;mysqld.address\u0026#34;, \u0026#34;mysql:3306\u0026#34;] environment: DATA_SOURCE_NAME: \u0026#34;exporter:MY-EXPORTER-PASSWORD@mysql:3306/\u0026#34; depends_on: - mysql volumes: - ./exporter_config.toml:/.my.cnf ports: - \u0026#34;9104:9104\u0026#34; It has 3 sections, one for each workload. Of course this shows example users and passwords, be secure when deploying into a real environment. The exporter config contains the following:\n[client] user = exporter password = MY-EXPORTER-PASSWORD The easiest way to install this as an \u0026ldquo;App\u0026rdquo; in TrueNAS is to use the include directive and point to the location of the app_compose.yaml file, as shown below.\nSaving the config starts the app.\nPost install configuration # At this point the both MySQL and phpMyAdmin should be up and running. However the Prometheus exporter will not work as there is no exporter database user. I logged into phpMyAdmin (via port 6060 on my TrueNAS server), and created the user, equivalent to the SQL statement in the mysqld_exporter docs\nCREATE USER \u0026#39;exporter\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;MY-EXPORTER-PASSWORD\u0026#39; WITH MAX_USER_CONNECTIONS 3; GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO \u0026#39;exporter\u0026#39;@\u0026#39;localhost\u0026#39;; After the user was created, I could start to see data at the metrics endpoint http://\u0026lt;my-truenas-server\u0026gt;:9104/metrics, which look like this sample\n# HELP mysql_global_status_buffer_pool_pages Innodb buffer pool pages by state. # TYPE mysql_global_status_buffer_pool_pages gauge mysql_global_status_buffer_pool_pages{state=\u0026#34;data\u0026#34;} 7168 mysql_global_status_buffer_pool_pages{state=\u0026#34;free\u0026#34;} 1024 mysql_global_status_buffer_pool_pages{state=\u0026#34;misc\u0026#34;} 0 # HELP mysql_global_status_bytes_received Generic metric from SHOW GLOBAL STATUS. # TYPE mysql_global_status_bytes_received untyped mysql_global_status_bytes_received 1.5961840888e+10 # HELP mysql_global_status_bytes_sent Generic metric from SHOW GLOBAL STATUS. # TYPE mysql_global_status_bytes_sent untyped mysql_global_status_bytes_sent 1.208720008e+09 # HELP mysql_global_status_commands_total Total number of executed MySQL commands. # TYPE mysql_global_status_commands_total counter mysql_global_status_commands_total{command=\u0026#34;admin_commands\u0026#34;} 7231 mysql_global_status_commands_total{command=\u0026#34;alter_db\u0026#34;} 0 mysql_global_status_commands_total{command=\u0026#34;alter_event\u0026#34;} 0 mysql_global_status_commands_total{command=\u0026#34;alter_function\u0026#34;} 0 I added the endpoint as a scrape target on my server that runs alloy:\nAlloy scrapes the endpoint every 15s and remote writes the data to my homelab Mimir instance Next, I created a grafana user with SELECT only permissions to use for a Grafana datasource. I also installed the MySQL Exporter Quickstart and Dashboard on the Grafana instance running on my TrueNAS.\nIn the next blog post I\u0026rsquo;ll cover ingesting IMDB\u0026rsquo;s public datasets into the database.\nCover photo by Pawel Czerwinski on Unsplash ","date":"7 September 2025","externalUrl":null,"permalink":"/blog/1757284330287-mysql-truenas/","section":"Blogs","summary":"","title":"Setting up a MySQL environment in TrueNAS","type":"blog"},{"content":"","date":"7 September 2025","externalUrl":null,"permalink":"/tags/truenas/","section":"Tags","summary":"","title":"Truenas","type":"tags"},{"content":"","date":"20 April 2025","externalUrl":null,"permalink":"/tags/inkscape/","section":"Tags","summary":"","title":"Inkscape","type":"tags"},{"content":"","date":"20 April 2025","externalUrl":null,"permalink":"/tags/kde/","section":"Tags","summary":"","title":"Kde","type":"tags"},{"content":"","date":"20 April 2025","externalUrl":null,"permalink":"/tags/linux/","section":"Tags","summary":"","title":"Linux","type":"tags"},{"content":"TL;DR This short post shows you how to create an application entry for AppImage\u0026rsquo;s in KDE. This makes it easier to pin the app to the taskbar and makes sure it shows up in KDE\u0026rsquo;s application launcher.\nThe Problem # You download an application and it is in the AppImage format. You run it, then later right-click and pin it to the taskbar. However sometimes there is no icon, and when you click on it again it won\u0026rsquo;t open because it can\u0026rsquo;t find the executable. You see a notification that shows it can\u0026rsquo;t find the app in /tmp/\u0026lt;some dir\u0026gt;.\nOther times, depending on how the app is packaged and initially configured, everything works right out of the box. This post shows you how to make a launcher for AppImage\u0026rsquo;s manually.\nWhat to do # As an example I\u0026rsquo;ll work through making a launcher for Inkscape, which prefers the AppImage format for Linux. Let\u0026rsquo;s assume that I\u0026rsquo;ve downloaded the file, Inkscape-e7c3feb-x86_64_0QCD8vJ.AppImage, and put it in my ~/apps/ folder.\nOpen the app, then see where it is mounted from the terminal\nLooking at the directory, there is a logo file (org.inkscape.Inkscape.png) and a desktop file (org.inkscape.Inkscape.png).\nEvery AppImage I have looked at has a logo and desktop file, now they just need to go somewhere the system will recognize them. On Linux systems this will be in the user\u0026rsquo;s ~/.local/ directory.\nMove the logo file to ~/.local/share/logos/ and the desktop file to ~/.local/share/applications/.\nYou may need to edit the Exec= using the full path to the AppImage file. In the case of Inkscape, the default desktop file was overly complex, so I made a new one with contents\n[Desktop Entry] Version=1.0 Name=Inkscape Categories=Graphics Terminal=false Icon=org.inkscape.Inkscape Exec=/home/vernon/apps/Inkscape-e7c3feb-x86_64_0QCD8vJ.AppImage The \u0026ldquo;Categories\u0026rdquo; parameter is important as that is where the app will show up in the system menu (for me it\u0026rsquo;s KDE\u0026rsquo;s application launcher). Run the update-desktop-database command with the local applications directory for the system to recognize the changes.\nupdate-desktop-database ~/.local/share/applications Now the launcher should be visible from the runner (Alt+F2) and system menus.\nAnd that\u0026rsquo;s it. There are tools for managing AppImage installations, but I use a small enough number of them that I prefer to manage things manually.\n","date":"20 April 2025","externalUrl":null,"permalink":"/blog/kde-appimage/","section":"Blogs","summary":"","title":"Manual Desktop Entries for AppImages","type":"blog"},{"content":"TL;DR On Hacker News today I came across an interesting blog post called Collatzeral Damage: Bitwise and Proof Foolish. It compared going through the sequence in a base-10 vs bitwise fashion. I was curious about the performance differences, so I did the comparison in both Go and Python. This post describes the results of that comparison.\nMethodology # Both of the programs for Go and Python were extremely simple, and I put them in this GitHub repo: https://github.com/aldernero/collatz-bitwise-blog-post\nThe key parts of each are below:\nGo\nfunc collatzRecursiveTraditional(n uint64) { if n \u0026lt;= 1 { return } if n%2 == 1 { collatzRecursiveTraditional(3*n + 1) } else { collatzRecursiveTraditional(n / 2) } } func collatzRecursiveBitwise(n uint64) { if n \u0026lt;= 1 { return } if (n \u0026amp; 1) == 1 { collatzRecursiveBitwise(((n \u0026lt;\u0026lt; 1) | 1) + n) } else { collatzRecursiveBitwise(n \u0026gt;\u0026gt; 1) } } Python\ndef collatz_recur_trad(n): if n == 1: return if n % 2 == 1: collatz_recur_trad(3*n + 1) else: collatz_recur_trad(n//2) def collatz_recur_bit(n): if n == 1: return if n \u0026amp; 1: collatz_recur_bit(((n \u0026gt;\u0026gt; 1) | 1) + n) else: collatz_recur_bit(n \u0026gt;\u0026gt; 1) The blog post covers a recursive approach, but in testing I also used an iterative approach. I ran all 4 combinations (traditional \u0026amp;\u0026amp; bitwise, recursive \u0026amp;\u0026amp; iterative) for both languages. I calculated the time to traverse the Collatz sequence for integers from 1 to 10 million.\nResults # The table below shows the elapsed time for all combinations.\nMethod Approach Go Python Traditional recursive 7.4s 147.2s Bitwise recursive 7.2s 29.8s Traditional iterative 2.4s 101.8s Bitwise iterative 2.4s 24.5s The results were mostly expected\niterative is faster than recursive For Go the performance was similar for bitwise and traditional approaches, but for Python bitwise is faster Golang is faster than Python (duh) I think the first point is expected since iterative is closer to what the CPU actually does to perform the calculation, and uses less memory.\nAt first I wasn\u0026rsquo;t sure what the performance difference between traditional and bitwise would be for Go, but given that it\u0026rsquo;s a compiled language I suppose all the base-10 arithmetic operations get turned into bitwise operations during compilation? \u0026#x1f937; For Python it makes sense that bitwise would be faster since it\u0026rsquo;s an interpreted language and using bitwise operations skips a step for the intepreter to do the conversion.\nFor Go we can isolate the comparison further by benchmarking the individual parts of the calcualtion:\nfunc BenchmarkCollatzOddTraditional(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { _ = 3*i + 1 } } func BenchmarkCollatzEvenTraditional(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { _ = i / 2 } } func BenchmarkCollatzOddBitwise(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { _ = ((i \u0026lt;\u0026lt; 1) | 1) + i } } func BenchmarkCollatzEvenBitwise(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { _ = i \u0026gt;\u0026gt; 1 } } Running the benchmark shows:\ngoos: linux goarch: amd64 pkg: collatzBitwise cpu: AMD Ryzen 7 3700X 8-Core Processor BenchmarkCollatzOddTraditional BenchmarkCollatzOddTraditional-16 1000000000\t0.2551 ns/op BenchmarkCollatzEvenTraditional BenchmarkCollatzEvenTraditional-16 1000000000\t0.2614 ns/op BenchmarkCollatzOddBitwise BenchmarkCollatzOddBitwise-16 1000000000\t0.2528 ns/op BenchmarkCollatzEvenBitwise BenchmarkCollatzEvenBitwise-16 1000000000\t0.2518 ns/op Summary # This was a fun little exercise. I would be interested to see comparisons for other languages, but I only had the patience to test these two. If you test some other language, let me know on one of the social platforms.\nReferences # https://news.ycombinator.com/item?id=43375353 - Hacker News article discussing the blog post Collatzeral Damage: Bitwise and Proof Foolish - blog post that inspired this post https://github.com/aldernero/collatz-bitwise-blog-post - GitHub repo with the code used in this post ","date":"15 March 2025","externalUrl":null,"permalink":"/blog/collatz-bitwise-comparison/","section":"Blogs","summary":"","title":"Collatz Bitwise Comparison","type":"blog"},{"content":"","date":"15 March 2025","externalUrl":null,"permalink":"/tags/golang/","section":"Tags","summary":"","title":"Golang","type":"tags"},{"content":"","date":"15 March 2025","externalUrl":null,"permalink":"/tags/math/","section":"Tags","summary":"","title":"Math","type":"tags"},{"content":"","date":"15 March 2025","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"8 March 2025","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"8 March 2025","externalUrl":null,"permalink":"/categories/development/","section":"Categories","summary":"","title":"Development","type":"categories"},{"content":"","date":"8 March 2025","externalUrl":null,"permalink":"/tags/generative-art/","section":"Tags","summary":"","title":"Generative Art","type":"tags"},{"content":"TL;DR Sketchy has been around for 3 years, but today marks its first official release, v0.1.0. Read below for more about the generative art framework for Go.\nWhat is Sketchy? # Sketchy is a framework for generative art, similar to Processing for Java, p5.js for JavaScript, vsketch for Python, and openFrameworks for C++. It provides a canvas and controls for quickly iterating on designs. It can save results in PNG or SVG format. The latter is what I mostly use as I typically plot my designs with my AxiDraw v3 pen plotter.\nWhen I needed to learn the Go programming language for a new job, I figured that creating something with Go that aligned with my favorite hobby (generative art) would make for quick language aquisition. As it was one of the first apps I wrote in Go, it was a little rough around the edges. Over the last few years I\u0026rsquo;ve been both improving Sketchy and using it to create most of my art. You can see all my art on my Instagram account, below are a few examples from a fractal design.\nRecently I realized that to increase adoption I ought to create a release, and spend more time on adding features and functions in a deliberate and planned way. The goal is to eventually get to a v1.0 release.\nWhat can Sketchy do? # Sketchy follows the familiar setup/update/draw pattern found in many generative art frameworks, shown below:\nThe configuration file describes things like the sketch dimensions and title, as well as controls like sliders, checkboxes, and buttons that can be referenced from code.\nIn the code itself the controls can be used like\n// get the value from a slider control sliderVal := s.Slider(\u0026#34;slider name\u0026#34;) // float64 // both checkboxes and buttons are Toggle structs toggleVal := s.Toggle(\u0026#34;checkbox/button name\u0026#34;) // bool Besides the user defined controls, there are some builtin controls that cover the random seed and saving files. The image below shows an example Control Panel\nThe github repo has more examples and a tutorial for creating a \u0026ldquo;Hello Circle\u0026rdquo; (instead of \u0026ldquo;Hello World\u0026rdquo;) project.\nWhat\u0026rsquo;s next? # I plan to add some new features in upcoming releases:\nAn install sketchy cli subcommand that would create a config file in the user\u0026rsquo;s home directory. A configuration file to store things like default colors, fonts, etc. that can be referenced in designs. Perhaps a way to design for very large outputs while showing a scaled down version while iterating. Maybe additional controls in the Control Panel like color controls. Like with any open source project, feel free to open issues in the github repo if you want to request features or run into any bugs.\nHappy Sketching!\n","date":"8 March 2025","externalUrl":null,"permalink":"/blog/introducing-sketchy/","section":"Blogs","summary":"","title":"Introducing Sketchy","type":"blog"},{"content":"","date":"8 March 2025","externalUrl":null,"permalink":"/tags/sketchy/","section":"Tags","summary":"","title":"Sketchy","type":"tags"},{"content":"This is an idea I came up with to better keep track of time spent on various activities.\nThe Problem # One of my former co-workers used to say\nThe days are long and the weeks are short.\nThis rings true in my experience. It seems like during the day I\u0026rsquo;m busy enough, but at the end of the week I look back with regret at how little I accomplished. I\u0026rsquo;m not worried about how much time was spent on urgent tasks and meaningless busy work. I get alarmed however when I consider how much time I\u0026rsquo;m not spending on the things that are meaningful to me.\nThis problem is not new for me either. In the past I\u0026rsquo;ve thought about using software to keep track of time spent on meaningful but non-urgent activities. I wanted to create something custom to fit my use case, so I started writing a TUI-based program in Go (Timebox). The project suffered from predictable scope creep, and required so much time to implement that it was a victim of the problem I\u0026rsquo;m trying to solve in the first place. I simply didn\u0026rsquo;t spend enough dedicated time on it to reach completion. The code is still a WIP, and I do hope to finish it at some point.\nThen over the holiday break a simple idea occurred to me. I have an 8oz Mason jar in my office that I use to hold pens. I thought, what if I get a bunch differently colored marbles, and transfer them from one mason jar to another to track time spent. The rest of the article describes the Time Jar system I came up with and how it worked out in practice for the first week.\nThe Idea # The key concept is that we have a finite amount of time per week. Not only are there just 168 hours, but a lot of that is already \u0026ldquo;pre-booked\u0026rdquo;. Sleep accounts for 56 hours, and work another 40. That leaves just 72 hours left.\nLet\u0026rsquo;s associate one marble with one hour. Conveniently then, a week\u0026rsquo;s worth of potential time for meaningful activities looks like the picture below, a roughly full jar of marbles. Let\u0026rsquo;s call this jar the \u0026ldquo;potential jar\u0026rdquo; Each color represents a different category of activity. The next section has a breakdown of the categories I wanted to focus on.\nA second mason jar is initially empty. Let\u0026rsquo;s call this the \u0026ldquo;realized jar\u0026rdquo; Throughout the week, after I spend an hour engaged in one of the tracked categories, I transfer a marble of the associated color from the potential jar to the realized jar. At the end of the week, hopefully the comparison between the two jars should give me some insights and feedback about how to adjust my time management.\nThe Categories # These are the categories I\u0026rsquo;m tracking. These are of course very personalized and are likely to change in the future. The marbles I bought came in six colors, so I picked six areas I\u0026rsquo;d like to spend more time on:\nArt - creating, working on my shop, learning new skills, etc Exercise - anything that gets me moving, I\u0026rsquo;ve been exceedingly sedentary since becoming a remote worker. Writing - anything really, stories, poems, or blog posts like this. Mindfulness - meditation, therapy, anything else that promotes better mental health Coding - coding projects that are not directly related to my generative art Music - I want to learn guitar and relearn the piano As an example of how terrible I am at spending time on these, let\u0026rsquo;s look at the last two. The time tracking app I mentioned earlier, Timebox, was initially created in August of 2022 and hasn\u0026rsquo;t had an update since March of 2024. There are plenty of other full-stack side projects in various states of incompleteness. On the music front, I\u0026rsquo;ve had a guitar for over a decade that I don\u0026rsquo;t how to play, and a digital piano I haven\u0026rsquo;t touched since I bought it a year or two ago.\nReflections # Even before starting the week, I realized something important. There should be no expectation that 100% of the marbles are transferred by the end of the week. There are so many things we have do that eat into the potential hours. Running errands, cooking, cleaning, hygiene, and simply transitioning from one activity on another take time that can really add up. In addition, there are lots of things I like to do but aren\u0026rsquo;t in any of the six categories because they are not things that help me develop or grow. For example I love video games and TV, and making time for them is good, but there should be limits. In fact, spending a certain amount of time enjoying those things would make for a good reward system to keep myself motivated to continue this practice. For example for each hour earned in the Time Jar, I could bank 30 minutes of time to spend watching TV.\nAnyway, getting to the results for the week, here is how the two jars looked\nThe table below shows the breakdown by category.\nCategory Hours art 4 exercise 6 mindfullness 2 coding 5 music 1 Total 18 In all, I spent 18 hours on meaningful activities. Since the potential maximum is 72 hours, this represents a 25% efficiency. It\u0026rsquo;s a low number, but at least it gives me a baseline to measure improvements against. What is the upper limit of efficiency? I have no clue, but I imagine doing better than 75% would be nearly impossible. We will see.\nAn additional insight arose towards the end of the week. As I was spending more time in some categories compared to others, the marbles on top tended to be for the less consumed categories. I needed to dig deeper to find the right color marble to transfer. It was almost like the jar was telling me to diversify my time more.\nWhat\u0026rsquo;s Next # I\u0026rsquo;m pleased with the way the first week went. Although I had a low efficiency (25%) I was surprised by how motivated I was to engage with these meaningful activities so I could transfer a marble and watch them fill up the realized jar. I will continue this practice, with the goal of improving my efficiency in this new week. I also plan to try something similar to track things at work, with 40-50 marbles in 2 separate jars. I\u0026rsquo;ll periodically post updates on my progress and how the practice has (hopefully) improved how I spend my time.\n","date":"29 January 2025","externalUrl":null,"permalink":"/blog/time-jars/","section":"Blogs","summary":"","title":"Time Jars","type":"blog"},{"content":"","date":"29 January 2025","externalUrl":null,"permalink":"/categories/time-management/","section":"Categories","summary":"","title":"Time Management","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]